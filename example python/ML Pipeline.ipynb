{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "05f735f8434a0ee824fac4a4332f2e30427b10f3",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "3f49e690bad668498749c20d2d5b3a78e24e943a"
      },
      "cell_type": "markdown",
      "source": "<h1>Introduction</h1>\n\nIn this tutorial I'd like to illustrate some advanced uses of pipelines. Some readers might have used them in work already, or could be totally unfamiliar with them - no worries, I'll cover basic uses as well as some advanced tricks."
    },
    {
      "metadata": {
        "_uuid": "b6af612936ca22d6ff0591c5db2bc69c6d3a25b2"
      },
      "cell_type": "markdown",
      "source": "<h2>Advantages of pipelines</h2>\n1. Use of pipelines gives you a kind of meta-language to describe your model and abstract from some implementation details.\n2. With pipelines, you don't need to carry test dataset transformation along with your train features - this is taken care of automatically.\n3. Hyperparameter tuning made easy - set new parameters on any estimator in the pipeline, and refit - in 1 line. Or use GridSearchCV on the pipeline."
    },
    {
      "metadata": {
        "_uuid": "42657533a6b3ebae9136478435578be7cede6732"
      },
      "cell_type": "markdown",
      "source": "<h2>Simple illustrations</h2>\n\nLet's start with simple illustrations."
    },
    {
      "metadata": {
        "_uuid": "e4b58373f1a2d0def912cc896e28e4fc364b5a17"
      },
      "cell_type": "markdown",
      "source": "<h3>Data preparation</h3>\n\nI assume you are familiar with the data structure from other hot tutorials, so I'll be brief here."
    },
    {
      "metadata": {
        "_uuid": "aa24090967ee023fed00f4ced0755daefd3ecc7b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#read the data in\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "_uuid": "5d400a2503be27e80cbea561f0bcff48432b617b",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#encode labels to integer classes\nfrom sklearn.preprocessing import LabelEncoder\n\nlb = LabelEncoder().fit(train['author'])\n\n#Original labels are stored in a class property\n#and binarized labels correspond to indexes of this array - 0,1,2 in our case of three classes\nlb.classes_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "49343b15f7170d17da7d998355a8b24110445658",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#after transformation the label will look like an array of integer taking values 0,1,2\nlb.transform(train['author'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e73c5a3a81e218da704fccfc260b6f6ac5816987"
      },
      "cell_type": "markdown",
      "source": "Split the thain dataset into two parts: the large one is used for training models,\nthe smaller one serves as a validation dataset - which is not seen during training, but has labels.\n\nHere our new testing data set will be 0.7 of original train dataset (test_size=0.3),\nwe want proportion of classes to be kept in the new test (stratify=train['author']), \nand we set the random state for reproducability (random_state=17)."
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "34aa089f6a5c5adedca3ae56a8737dd074f7c71e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nX_train_part, X_valid, y_train_part, y_valid =\\\n    train_test_split(train['text'], \n                     lb.transform(train['author']), \n                test_size=0.3,random_state=17, stratify=train['author'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "419b2ce184dc120ba3904bd1c9863b614f88b0e5"
      },
      "cell_type": "markdown",
      "source": "<h3>Preparing a pipeline</h3>\n\nLet's create our fist model."
    },
    {
      "metadata": {
        "_uuid": "4d9dfe4e6a86527fd0b521176fe4c04fd584f85f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.linear_model import LogisticRegression\n\npipe1 = Pipeline([\n    ('cv', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    ('logit', LogisticRegression()),\n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "44969bdc517c01f4ec17c885175cc3d7792b0df9"
      },
      "cell_type": "markdown",
      "source": "In this pipeline, input data comes first to `CountVectorizer`, which creates a sparse matrix of word counts in each sentence. This matrix then serves as input to `TfidfTransformer` which massages the data and handles it to the LogisticRegression estimator for training and prediction."
    },
    {
      "metadata": {
        "_uuid": "4c475c18faf54ab308faf699b8cbb1abcc2c2467"
      },
      "cell_type": "markdown",
      "source": "<h3>Fitting the Model</h3>\n\nOur pipe1 object has all the properties of an estimator, so we can treat it as such. Hence, we call the `fit()` method.\n\nNote that Pipeline \"knows\" that the first tho steps are transformers, so it will only call `fit()` and `transform()` on them, or just `fit_transform()` if it's defined for the class. For the `LogisticRegression` instance - our final model - only `fit()` will be called."
    },
    {
      "metadata": {
        "_uuid": "d7752d7cc3adce40d90794accffbefa4886fe9e8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pipe1.fit(X_train_part, y_train_part)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ae6eff16557599c56ef2d046131b468156a08483"
      },
      "cell_type": "markdown",
      "source": "Let's stop here for a moment and check what we've got. \n\n\nWe can look up all the steps of the pipeline, and all the parameters of the steps: "
    },
    {
      "metadata": {
        "scrolled": true,
        "_uuid": "7323640062b4de84356628bad3da014af8ddf92e",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pipe1.steps",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6ef089635561befeb65c364c0014120867edd5b7"
      },
      "cell_type": "markdown",
      "source": "We can access each step's parameters by name, as well as any of its methods and properties:"
    },
    {
      "metadata": {
        "scrolled": true,
        "_uuid": "8c45b831ca2dd0036a848e70cdcd9593b4eed3ac",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pipe1.named_steps['logit'].coef_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "15686f8338d0ce68e127bf1afa4dca6b660dc599"
      },
      "cell_type": "markdown",
      "source": "<h3>Making predictions</h3>\n\nThis is as easy as with a 'regular' model. We just call `predict()` or `predict_proba()`.\nLet's use our hold-out data for validation:"
    },
    {
      "metadata": {
        "_uuid": "6fe13bf59e8d09d95772ea50e755272843b7ddf6",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import log_loss\n\npred = pipe1.predict_proba(X_valid)\nlog_loss(y_valid, pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ed5bdf5cbf0e92a48256e573ff26741dd0309c15"
      },
      "cell_type": "markdown",
      "source": "<h3>Playing with parameters</h3>\nThat was not a winner! But hold on, we are not there yet!\nWe can improve the score by tuning some parameters. As I have shown earlier, we can check every step's paramters by its name:"
    },
    {
      "metadata": {
        "_uuid": "1a33d9b0b2ee4f21d90546b5d6807d68028b9767",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pipe1.named_steps['logit'].get_params()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "23eec3c0b7e5b211f5c976f09256831f21ec8277"
      },
      "cell_type": "markdown",
      "source": "But we can also check and set them all at once:"
    },
    {
      "metadata": {
        "_uuid": "022ca3bdf1fd9d5f33e6bda35211cde459157735",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pipe1.get_params()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "9ff444c4502f5414623adcd81431dae35fdd200e"
      },
      "cell_type": "markdown",
      "source": "You can see here, that the Pipeline class has all steps' parameters with their respective names prepended. We can set them as well and fit the model."
    },
    {
      "metadata": {
        "_uuid": "99556783de51beece20cc11af8850ca972dfdca8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#set_params(cv__lowercase=True)\npipe1.set_params(cv__min_df=6, \n                 cv__lowercase=False).fit(X_train_part, y_train_part)\npred = pipe1.predict_proba(X_valid)\nlog_loss(y_valid, pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "764649712867e82179411300bb7c2e2f94ea2f88"
      },
      "cell_type": "markdown",
      "source": "A little bit better! You get the idea. Thinking `GridSearchCV` or `cross_val_score`? Yes, will work on the pipeline too. Fork this kernel and implement it yourself!"
    },
    {
      "metadata": {
        "_uuid": "edf68f1b3dc358027ec4b5fc9ab52dcfa6eae477"
      },
      "cell_type": "markdown",
      "source": "<h3>Playing with a model</h3>\nWould you like to try another classifier? Naive Bayes seems to be in favor across winning kernels. Replacing a pipeline step is easy:"
    },
    {
      "metadata": {
        "_uuid": "cc5dbac95f9dc62ffbaa6f20795c62fedbbbbd54",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n\npipe1 = Pipeline([\n    ('cv', CountVectorizer()),\n    ('tfidf', TfidfTransformer()),\n    #('logit', LogisticRegression()),\n    ('bnb', BernoulliNB()),\n   \n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "4786c132dd506439d8e17ef4cda7e574b74604a9",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pipe1.fit(X_train_part, y_train_part)\npred = pipe1.predict_proba(X_valid)\nlog_loss(y_valid, pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "18e4c1fba1337c86a50b5e49c7978971dd90e9ba"
      },
      "cell_type": "markdown",
      "source": "Best score so far! Have more ideas? Fork this kernel and try them out!\n\nAlso, for more examples and a gentle intro, read another great Spooky pipeline tutorial: [pipeline for the beginners](https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines)\n"
    },
    {
      "metadata": {
        "_uuid": "697eeb65069896df6d7ac9ce20fa78769ca5f393"
      },
      "cell_type": "markdown",
      "source": "<h3>Feature Union</h3>\n\nAnother strong side of pipelines come from its brother class - `FeatureUnion`. It will help us to combine together some new features that we create as part of EDA. Let's, for example, take a statistics on parts of speech used in each sentence,  and see if it can help to improve the score."
    },
    {
      "metadata": {
        "_uuid": "add801ed525661ffd6e6c399108e9da34995b83c"
      },
      "cell_type": "markdown",
      "source": "<h3>NLTK Part-of-Speech tagger</h3>\n\nSuppose we assume that authors could be distinguished by some statistics of use of some parts of speech. May be frequency of conjugatoin is a significant feature? Or use of punctuation?\n\nNLTK can help to tag words in sentences."
    },
    {
      "metadata": {
        "_uuid": "35bf44ab053002f58e43120908d7cb1c5db10076",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import nltk\n\ntext = \"And now we are up for 'something' completely different;\"\ntokens = nltk.word_tokenize(text)\ntagged = nltk.pos_tag(tokens)\ntagged",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0196bccf0ec90408351ed2271e5a4a1e01298048"
      },
      "cell_type": "markdown",
      "source": "Puzzled about all the tags? `CC` means conjunction, coordinated. Take a look at the complete description with `nltk.help.upenn_tagset()` that I don't run here to keep the clutter down.\n"
    },
    {
      "metadata": {
        "_uuid": "c71c1d9493ecb99058b8e8e6581a9c745704365a"
      },
      "cell_type": "markdown",
      "source": "So, in order to tag our text in the pipeline, we will create an estimator class of our own. Don't be afraid - this is simple. We just have to inherit some base classes and overload very few functions that we are actually going to use:"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "395b773de224fcb9c233ea9e7e97ee6bad40a0f1",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.base import BaseEstimator, TransformerMixin\nfrom collections import Counter\n\nclass PosTagMatrix(BaseEstimator, TransformerMixin):\n    #normalise = True - devide all values by a total number of tags in the sentence\n    #tokenizer - take a custom tokenizer function\n    def __init__(self, tokenizer=lambda x: x.split(), normalize=True):\n        self.tokenizer=tokenizer\n        self.normalize=normalize\n\n    #helper function to tokenize and count parts of speech\n    def pos_func(self, sentence):\n        return Counter(tag for word,tag in nltk.pos_tag(self.tokenizer(sentence)))\n\n    # fit() doesn't do anything, this is a transformer class\n    def fit(self, X, y = None):\n        return self\n\n    #all the work is done here\n    def transform(self, X):\n        X_tagged = X.apply(self.pos_func).apply(pd.Series).fillna(0)\n        X_tagged['n_tokens'] = X_tagged.apply(sum, axis=1)\n        if self.normalize:\n            X_tagged = X_tagged.divide(X_tagged['n_tokens'], axis=0)\n\n        return X_tagged",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "5d5011d21e15155244676497509d05bfd8a0df33"
      },
      "cell_type": "markdown",
      "source": "Now, our new pipeline:"
    },
    {
      "metadata": {
        "_uuid": "bcf2ba2a339857374ae87de274a2536985a9a266",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from sklearn.pipeline import FeatureUnion\n\npipe2 = Pipeline([\n    ('u1', FeatureUnion([\n        ('tfdif_features', Pipeline([\n            ('cv', CountVectorizer()),\n            ('tfidf', TfidfTransformer()),\n        ])),\n        ('pos_features', Pipeline([\n            ('pos', PosTagMatrix(tokenizer=nltk.word_tokenize) ),\n        ])),\n    ])),\n    ('logit', LogisticRegression()),\n\n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "14002e72aef21345b920c46c4b99c4427c74a338",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pipe2.fit(X_train_part, y_train_part)\npred = pipe2.predict_proba(X_valid)\nlog_loss(y_valid, pred)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e73eee279e9b1bbe8fb9d4b14cc42ad354264fbb"
      },
      "cell_type": "markdown",
      "source": "Not an improvements, but hey, we learned somthing new!\n\nBy this cell, the reader may already feel the power on the new instruments. Are there downsides? Read on."
    },
    {
      "metadata": {
        "_uuid": "76ce719e8146b8799c9c4f01cfd6251df2fb1877"
      },
      "cell_type": "markdown",
      "source": "<h2>What gets stuck in the pipes?</h2>\n\nOk, someone may comment - the way `CounterVectorizer` is usually fit is on the combined train+test datasets, so that the entire vocabulary is learnt. And pipeline accepts one dataset at a time. Yes, this is a problem. The `vocabulary` option won't help us, in case we want to play with ngrams. Is there a work around? Yes. Read on the advanced section\n\nWhat about stacking? There are some complex worklfows, for example in [Simple Feature Engg Notebook - Spooky Author](https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author) it is proposed to stack 7 models! Yes, we can do so with piplines, read on.\n\n\nIt could be slow to run all transformations all over again! True, and I'll show you the way to save time.\n"
    },
    {
      "metadata": {
        "_uuid": "c58b714f54ba2028351e2ff522f54bf1d8e876b7"
      },
      "cell_type": "markdown",
      "source": "<h3>Overloading CountVectorizer class</h3>"
    },
    {
      "metadata": {
        "_uuid": "27887df4562c480c20d13af91966a56147130031",
        "trusted": false
      },
      "cell_type": "code",
      "source": "class CountVectorizerPlus(CountVectorizer):\n    def __init__(self, *args, fit_add=None, **kwargs):\n        #this will store a reference to an extra data to include for fitting only\n        self.fit_add = fit_add\n        super().__init__(*args, **kwargs)\n    \n    def transform(self, X):\n        U = super().transform(X)\n        return U\n    \n    def fit_transform(self, X, y=None):\n        if self.fit_add is not None:\n            X_new = pd.concat([X, self.fit_add])\n        else:\n            X_new = X\n        #calling CountVectorizer.fit_transform()\n        super().fit_transform(X_new, y)\n\n        U = self.transform(X)\n        return U\n    ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "f43753a48bce71d2f63926812a78873ecfde48c0",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pipe1a = Pipeline([\n    ('cv', CountVectorizerPlus(fit_add=test['text'])),\n    #('cv', CountVectorizerPlus()),\n    ('tfidf', TfidfTransformer()),\n    #('logit', LogisticRegression()),\n    ('bnb', BernoulliNB()),\n   \n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d896212ce3d997d4099412c6cf92c1e133a34355",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pipe1a.fit(X_train_part, y_train_part)\npred = pipe1a.predict_proba(X_valid)\nprint(log_loss(y_valid, pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "26104a78b1dbc021835680217cd6932033df1080"
      },
      "cell_type": "markdown",
      "source": "<h3>Stacking with Pipelines</h3>\n\nIf you now try to fit the following pipeline below, with intermediate classifiers, whose output you would like to combine and pass onto the final Classifier, it's going to fail. Why? Pipeline does not like to have more than one final estimator. After all, it's called final because, well, it run as a final step in the pipeline.\n"
    },
    {
      "metadata": {
        "_uuid": "15fcaa424498793b7c476887360cb88fa283b2d2"
      },
      "cell_type": "markdown",
      "source": "_This cell is Markdown, so the kernel won't stop here._\n```\npipe3 = Pipeline([\n    ('u1', FeatureUnion([\n        ('tfdif_features', Pipeline([\n            ('cv', CountVectorizer()),\n            ('tfidf', TfidfTransformer()),\n            ('tfidf_logit', LogisticRegression()),\n        ])),\n        ('pos_features', Pipeline([\n            ('pos', PosTagMatrix(tokenizer=nltk.word_tokenize) ),\n            ('pos_logit', LogisticRegression()),\n        ])),\n    ])),\n    ('xgb', XGBClassifier()),\n\n])\n```"
    },
    {
      "metadata": {
        "_uuid": "9cb492f1e985e076cad73d8678d6b21c9a52aad0"
      },
      "cell_type": "markdown",
      "source": "Happily, there is a solution. We can _pretend_ that our classifier is a transformer class, while it will 'transform' the input data into class predictions. For this, we make a wrapper around an estimator class:"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "be767760233be0eed6dd5f3f7f4ac8eba0c44821",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#stacking trick\nfrom sklearn.metrics import get_scorer\nclass ClassifierWrapper(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, estimator, verbose=None, fit_params=None, use_proba=True, scoring=None):\n        self.estimator = estimator\n        self.verbose = verbose #True = 1, False = 0, 1 - moderately verbose, 2- extra verbose    \n        if verbose is None:\n            self.verbose=0\n        else:\n            self.verbose=verbose\n        self.fit_params= fit_params\n        self.use_proba = use_proba #whether to use predict_proba in transform\n        self.scoring = scoring # calculate validation score, takes score function name\n        #TODO check if scorer imported?\n        self.score = None #variable to keep the score if scoring is set.\n\n    def fit(self,X,y):\n        fp=self.fit_params\n        if self.verbose==2: print(\"X: \", X.shape, \"\\nFit params:\", self.fit_params)\n        \n        if fp is not None:\n            self.estimator.fit(X,y, **fp)\n        else:\n            self.estimator.fit(X,y)\n        \n        return self\n    \n    def transform(self, X):\n        if self.use_proba:\n            return self.estimator.predict_proba(X) #[:, 1].reshape(-1,1)\n        else:\n            return self.estimator.predict(X)\n    \n    def fit_transform(self,X,y,**kwargs):\n        self.fit(X,y)\n        p = self.transform(X)\n        if self.scoring is not None:\n            self.score = eval(self.scoring+\"(y,p)\")\n            #TODO print own instance name?\n            if self.verbose >0: print(\"score: \", self.score) \n        return p\n    \n    def predict(self,X):\n        return self.estimator.predict(X)\n    \n    def predict_proba(self,X):\n        return self.estimator.predict_proba(X)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "0e83c4a45662059aea007f010cb28925e2630d97",
        "trusted": false
      },
      "cell_type": "code",
      "source": "from xgboost import XGBClassifier\n#params are from the above mentioned tutorial\nxgb_params={\n    'objective': 'multi:softprob',\n    'eta': 0.1,\n    'max_depth': 3,\n    'silent' :1,\n    'num_class' : 3,\n    'eval_metric' : \"mlogloss\",\n    'min_child_weight': 1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.3,\n    'seed':17,\n    'num_rounds':2000,\n}",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e4f99989559076b24b06ae59e530250a12e18395",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pipe3 = Pipeline([\n    ('u1', FeatureUnion([\n        ('tfdif_features', Pipeline([\n            ('cv', CountVectorizer()),\n            ('tfidf', TfidfTransformer()),\n            ('tfidf_logit', ClassifierWrapper(LogisticRegression())),\n        ])),\n        ('pos_features', Pipeline([\n            ('pos', PosTagMatrix(tokenizer=nltk.word_tokenize) ),\n            ('pos_logit', ClassifierWrapper(LogisticRegression())),\n        ])),\n    ])),\n    ('xgb', XGBClassifier(**xgb_params)),\n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "_uuid": "256bdc6d674a73a587691f7a13eb80896550e44d",
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\npipe3.fit(X_train_part, y_train_part)\npred = pipe3.predict_proba(X_valid)\nprint(log_loss(y_valid, pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "75048d123447574b64b9cdc176e6fc2f54a94a4b"
      },
      "cell_type": "markdown",
      "source": "<h3>Caching pipeline results</h3>\n\nThis is possible with the `memory` parameter of the `Pipeline()` constructor. The argument is either path to a directory, or a `joblib` object."
    },
    {
      "metadata": {
        "_uuid": "6376066cc683ebbedbb23cc1fc5f24d9083a9d83",
        "trusted": false
      },
      "cell_type": "code",
      "source": "pipe4 = Pipeline([\n    ('u1', FeatureUnion([\n        ('tfdif_features', Pipeline([\n            ('cv', CountVectorizer()),\n            ('tfidf', TfidfTransformer()),\n            ('tfidf_logit', ClassifierWrapper(LogisticRegression())),\n        ], memory=\"/tmp\")),\n        ('pos_features', Pipeline([\n            ('pos', PosTagMatrix(tokenizer=nltk.word_tokenize) ),\n            ('pos_logit', ClassifierWrapper(LogisticRegression())),\n        ], memory=\"/tmp\")),\n    ])),\n    ('xgb', XGBClassifier(**xgb_params)),\n])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0b66a174fba4fc047dfe1aef5936e15653b7bb7b"
      },
      "cell_type": "markdown",
      "source": "**I run the same code twice - first time to fit&cache, second time to use cache only**\n\nNotice the difference! I'd like to warn you however. The cache may not always get invalidated when you think it should. You may want to manually remove the directory of the cache."
    },
    {
      "metadata": {
        "_uuid": "c6490d30d68363fb782ddb32b6f25c1b3b1a579a",
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\npipe4.fit(X_train_part, y_train_part)\npred = pipe4.predict_proba(X_valid)\nprint(log_loss(y_valid, pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d6100d28b20cce75ee926b670174047db9e96ed8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "%%time\npipe4.fit(X_train_part, y_train_part)\npred = pipe4.predict_proba(X_valid)\nprint(log_loss(y_valid, pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "633b2d84ff745cca1abbeecaaa40ce03b75aa9b7"
      },
      "cell_type": "markdown",
      "source": "<h2>Submission</h2>\n\nHere we show how easy is it to train the model on the full train dataset and generate predictions for the test one."
    },
    {
      "metadata": {
        "_uuid": "198800b924f1e0b44b72aa0ec7bfa98c27757825",
        "trusted": false
      },
      "cell_type": "code",
      "source": "#refit on the full train dataset\npipe4.fit(train['text'], lb.transform(train['author']))\n\n# obtain predictions\npred = pipe4.predict_proba(test['text'])\n\n#id,EAP,HPL,MWS\n#id07943,0.33,0.33,0.33\n#...\npd.DataFrame(dict(zip(lb.inverse_transform(range(pred.shape[1])),\n                      pred.T\n                     )\n                 ),index=test.id).to_csv(\"submission.csv\", index_label='id')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "7e21b09a3613760da88e253119ea16731ca81714"
      },
      "cell_type": "markdown",
      "source": "<h2>Conclusions and further reading</h2>\n\nOk, we didn't win, but I din't promice. :) I'll stop here and let the reader add his/her own features and models, stack them and hopefully, rocket to the top!\n\nWhat else can you learn about pipelines?\n\n- Go to the doc page, http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html, and check out some examples linked to at the end of the page.\n\n- Take a look at these tutorial and vote them up, if you like them.\n    - [pipeline for the beginners](https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines)\n    - [Simple Feature Engg Notebook - Spooky Author](https://www.kaggle.com/sudalairajkumar/simple-feature-engg-notebook-spooky-author) - try to implement all the models in one pipeline.\n- Fork this kernel and explore your own ideas!\n\n<h2>Good luck!</h2>\n"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "d0f55815b0c7f31fe3614c825fef2291fd6ac7f5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}