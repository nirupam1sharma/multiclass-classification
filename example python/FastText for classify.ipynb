{
  "cells": [
    {
      "metadata": {
        "_uuid": "5484dedb6db529f4dc58d0cb433186b3d823f429",
        "_cell_guid": "73969602-c633-450e-9364-9481a21a879c"
      },
      "cell_type": "markdown",
      "source": "# **This notebook's best result: val_acc is 0.8779, val_loss is 0.3129**"
    },
    {
      "metadata": {
        "_uuid": "7ccc3b4516a4fcde346a162ae4f9461016bbfbbf",
        "_cell_guid": "d5736ce6-a0b0-4cf0-beaf-a73837398da9"
      },
      "cell_type": "markdown",
      "source": "# **1. Few Preprocessings**\n# **2. Model: FastText by Keras**\n## **2.1** Change Preprocessings:\n- Do lower case "
    },
    {
      "metadata": {
        "_kg_hide-input": false,
        "_uuid": "b05ef71268db76a4e2565177bf6a5668a5fc428e",
        "_kg_hide-output": true,
        "_cell_guid": "93e00783-a024-4e87-a5e1-6709cb8cc981",
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\n\nimport pandas as pd\n\nfrom collections import defaultdict\n\nimport keras\nimport keras.backend as K\nfrom keras.layers import Dense, GlobalAveragePooling1D, Embedding\nfrom keras.callbacks import EarlyStopping\nfrom keras.models import Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\n\nfrom sklearn.model_selection import train_test_split\n\nnp.random.seed(7)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d700f739101e37903112e1de293323dcfbb577be",
        "collapsed": true,
        "_cell_guid": "a5cc2c3e-7960-482e-b548-c447b89925ec",
        "trusted": false
      },
      "cell_type": "code",
      "source": "df = pd.read_csv('./../input/train.csv')\na2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\ny = np.array([a2c[a] for a in df.author])\ny = to_categorical(y)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a01bab31ed7b8a55820612063576963488d99eb6",
        "_cell_guid": "a45cb3ba-d1bc-48e0-956c-27d0f49a9943"
      },
      "cell_type": "markdown",
      "source": "# 1. **Few Preprocessings**\n\nIn traditional NLP tasks, preprocessings play an important role, but...\n\n## **Low-frequency words**\nIn my experience, fastText is very fast, but I need to delete rare words to avoid overfitting.\n\n**NOTE**:\nSome keywords are rare words, such like *Cthulhu* in *Cthulhu Mythos* of *Howard Phillips Lovecraft*.\nBut these are useful for this task.\n\n## **Removing Stopwords**\n\nNothing.\nTo identify author from a sentence, some stopwords play an important role because one has specific usages of them.\n\n## **Stemming and Lowercase**\n\nNothing.\nThis reason is the same for stopwords removing.\nAnd I guess some stemming rules provided by libraries is bad for this task because all author is the older author.\n\n## **Cutting long sentence**\n\nToo long documents are cut.\n\n## **Punctuation**\n\nBecause I guess each author has unique punctuations's usage in the novel, I separate them from words.\n\ne.g. `Don't worry` -> `Don ' t worry`\n\n## **Is it slow?**\n\nDon't worry! FastText is a very fast algorithm if it runs on CPU. "
    },
    {
      "metadata": {
        "_uuid": "0023cd1542d866d931deb8472f8a0d6fb0262d9a",
        "_cell_guid": "8182b25a-f490-4b41-9865-ee1c04afecee"
      },
      "cell_type": "markdown",
      "source": "# **Let's check character distribution per author**"
    },
    {
      "metadata": {
        "_uuid": "246a428ca3a063294c15c8c08d234ecf01e4ddbb",
        "_kg_hide-output": true,
        "_cell_guid": "c1d00b0d-90e0-4f19-842c-51a82de42a10",
        "trusted": false
      },
      "cell_type": "code",
      "source": "counter = {name : defaultdict(int) for name in set(df.author)}\nfor (text, author) in zip(df.text, df.author):\n    text = text.replace(' ', '')\n    for c in text:\n        counter[author][c] += 1\n\nchars = set()\nfor v in counter.values():\n    chars |= v.keys()\n    \nnames = [author for author in counter.keys()]\n\nprint('c ', end='')\nfor n in names:\n    print(n, end='   ')\nprint()\nfor c in chars:    \n    print(c, end=' ')\n    for n in names:\n        print(counter[n][c], end=' ')\n    print()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8e72d6f22587780364ed24cae13ece4a403479dd",
        "_cell_guid": "7a3fdf4e-039d-4c93-bc21-9bad7dfc6ff8"
      },
      "cell_type": "markdown",
      "source": "# **Summary of character distribution**\n\n- HPL and EAP used non ascii characters like a `Ã¤`.\n- The number of punctuations seems to be good feature\n"
    },
    {
      "metadata": {
        "_uuid": "fee49fd9139b78ae03603d7d37eafa38f3cb29dc",
        "_cell_guid": "ce97fc0a-b85c-4f34-92c5-ae66a0730ace"
      },
      "cell_type": "markdown",
      "source": "# **Preprocessing**\n\nMy preproceeings are \n\n- Separate punctuation from words\n- Remove lower frequency words ( <= 2)\n- Cut a longer document which contains `256` words"
    },
    {
      "metadata": {
        "_uuid": "999012010cd8b9b20d3c5b16c11a2374a5ce44c0",
        "collapsed": true,
        "_cell_guid": "72ff2ff5-0945-4f39-8b02-39e4d5df16c5",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def preprocess(text):\n    text = text.replace(\"' \", \" ' \")\n    signs = set(',.:;\"?!')\n    prods = set(text) & signs\n    if not prods:\n        return text\n\n    for sign in prods:\n        text = text.replace(sign, ' {} '.format(sign) )\n    return text",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "53f325a090a44f7109f0537022398797704cdc80",
        "collapsed": true,
        "_cell_guid": "f123742f-540f-438d-aba3-ebbca69235be",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def create_docs(df, n_gram_max=2):\n    def add_ngram(q, n_gram_max):\n            ngrams = []\n            for n in range(2, n_gram_max+1):\n                for w_index in range(len(q)-n+1):\n                    ngrams.append('--'.join(q[w_index:w_index+n]))\n            return q + ngrams\n        \n    docs = []\n    for doc in df.text:\n        doc = preprocess(doc).split()\n        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n    \n    return docs",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "150f9f6643e6753386b2021ac812ecc0cac66202",
        "collapsed": true,
        "_cell_guid": "888047de-806e-4ad2-9fff-18b4d6583d30",
        "trusted": false
      },
      "cell_type": "code",
      "source": "min_count = 2\n\ndocs = create_docs(df)\ntokenizer = Tokenizer(lower=False, filters='')\ntokenizer.fit_on_texts(docs)\nnum_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n\ntokenizer = Tokenizer(num_words=num_words, lower=False, filters='')\ntokenizer.fit_on_texts(docs)\ndocs = tokenizer.texts_to_sequences(docs)\n\nmaxlen = 256\n\ndocs = pad_sequences(sequences=docs, maxlen=maxlen)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b9e353b548b0dfbd4b42a40d8a2643efeb359a20",
        "_cell_guid": "f9ebc033-2a26-4656-9472-8990c1a27c79"
      },
      "cell_type": "markdown",
      "source": "# **2. Model: FastText by Keras**\n\nFastText is very fast and strong baseline algorithm for text classification based on Continuous Bag-of-Words model a.k.a Word2vec.\n\nFastText contains only three layers:\n\n1. Embeddings layer: Input words (and word n-grams) are all words in a sentence/document\n2. Mean/AveragePooling Layer: Taking average vector of Embedding vectors\n3. Softmax layer\n\nThere are some implementations of FastText:\n\n- Original library provided by Facebook AI research: https://github.com/facebookresearch/fastText\n- Keras: https://github.com/fchollet/keras/blob/master/examples/imdb_fasttext.py\n- Gensim: https://radimrehurek.com/gensim/models/wrappers/fasttext.html\n\nOriginal Paper: https://arxiv.org/abs/1607.01759 : More detail information about fastText classification model"
    },
    {
      "metadata": {
        "_uuid": "8b56b2ef90e519b939b7bf9ec5a146f749807b02",
        "_cell_guid": "636eb75e-6fba-413e-996d-1395609b422c"
      },
      "cell_type": "markdown",
      "source": "# My FastText parameters are:\n\n- The dimension of word vector is 20\n- Optimizer is `Adam`\n- Inputs are words and word bi-grams\n  - you can change this parameter by passing the max n-gram size to argument of `create_docs` function.\n"
    },
    {
      "metadata": {
        "_uuid": "bba1d1a6416876e74ed688f56e4d5bc4990ec12a",
        "collapsed": true,
        "_cell_guid": "393d1ddb-0a87-42a3-8575-53ff7abff1da",
        "trusted": false
      },
      "cell_type": "code",
      "source": "input_dim = np.max(docs) + 1\nembedding_dims = 20",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "e6c16572e6b32923af39dfd29467e32b52561bb1",
        "collapsed": true,
        "_cell_guid": "2e3e1e3e-22f4-4727-ba6c-67f7b3e80d2f",
        "trusted": false
      },
      "cell_type": "code",
      "source": "def create_model(embedding_dims=20, optimizer='adam'):\n    model = Sequential()\n    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n    model.add(GlobalAveragePooling1D())\n    model.add(Dense(3, activation='softmax'))\n\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizer,\n                  metrics=['accuracy'])\n    return model",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "_kg_hide-output": true,
        "_cell_guid": "0db889db-0b3e-4025-8847-e3eb5f853f37",
        "_kg_hide-input": false,
        "_uuid": "22e57e010206a3044adf7b82160c7c3ca78030f8",
        "trusted": false
      },
      "cell_type": "code",
      "source": "epochs = 25\nx_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n\nmodel = create_model()\nhist = model.fit(x_train, y_train,\n                 batch_size=16,\n                 validation_data=(x_test, y_test),\n                 epochs=epochs,\n                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "19d0437ebb01a6f129fea39ba06c165b34e768cc"
      },
      "cell_type": "markdown",
      "source": "### **Result**\n\n- Best val_loss is 0.3409\n- Best val_acc is 0.8700\n\n"
    },
    {
      "metadata": {
        "_uuid": "1df8fe493c4dde44e1f1f0660546b641dcc06580"
      },
      "cell_type": "markdown",
      "source": "# **2.1 Change Preprocessings**\n\nNext, I change some parameters and preprocessings to improve fastText model.\n## **2.1.1 Do lower case**"
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "a41a24b4f9a9603f686b5a7accc84bdf200c02dc",
        "trusted": false
      },
      "cell_type": "code",
      "source": "docs = create_docs(df)\ntokenizer = Tokenizer(lower=True, filters='')\ntokenizer.fit_on_texts(docs)\nnum_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n\ntokenizer = Tokenizer(num_words=num_words, lower=True, filters='')\ntokenizer.fit_on_texts(docs)\ndocs = tokenizer.texts_to_sequences(docs)\n\nmaxlen = 256\n\ndocs = pad_sequences(sequences=docs, maxlen=maxlen)\n\ninput_dim = np.max(docs) + 1",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "bf365944c9acd5588f3076e0cf98e05fd70f6687",
        "trusted": false
      },
      "cell_type": "code",
      "source": "epochs = 16\nx_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.2)\n\nmodel = create_model()\nhist = model.fit(x_train, y_train,\n                 batch_size=16,\n                 validation_data=(x_test, y_test),\n                 epochs=epochs,\n                 callbacks=[EarlyStopping(patience=2, monitor='val_loss')])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "810225b280fcc7f8585f44aec81ad2e637645550"
      },
      "cell_type": "markdown",
      "source": "**Result**\n\n- Best val_loss is 0.3129\n- Best val_acc is 0.8787"
    },
    {
      "metadata": {
        "_uuid": "c1778eea905707a504de407eaf5325798f9d1e54",
        "trusted": false
      },
      "cell_type": "code",
      "source": "test_df = pd.read_csv('../input/test.csv')\ndocs = create_docs(test_df)\ndocs = tokenizer.texts_to_sequences(docs)\ndocs = pad_sequences(sequences=docs, maxlen=maxlen)\ny = model.predict_proba(docs)\n\nresult = pd.read_csv('../input/sample_submission.csv')\nfor a, i in a2c.items():\n    result[a] = y[:, i]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "22c59074fd51398d27b863e7323b8a744dd5d38c",
        "trusted": false
      },
      "cell_type": "code",
      "source": "result.to_csv('fastText_result.csv', index=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "collapsed": true,
        "_uuid": "8c184610e777b6be3bafdf10f6c689b92d9ea2ff",
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}